---
title: "W4HW1"
author: "David Futran"
date: "July 5, 2017"
output: html_document
---

```{r }
install.packages("e1071")
library(e1071)

```

```{r }
p_hincome_buy=.5
p_buy=.5
p_income_high=.5

p_buy_hincome=p_hincome_buy*p_buy/p_income_high
p_buy_hincome


```

```{r }
train <- data.frame(buy=c("yes","no","no","yes"), 
                    income=c("high","high","medium","low"))
classifier <- naiveBayes(buy ~ income,train)
classifier
```

```{r }
test <- data.frame(income=c("high"))
test$income <- factor(test$income, levels=c("high","medium", "low"))
prediction <- predict(classifier, test ,type="raw")
prediction

```

```{r }
train <- data.frame(buy=c("yes","no","no","yes"), 
                    income=c("high","high","medium","low"),
                    gender=c("male","female","female","male"))
classifier <- naiveBayes(buy ~ income+gender,train)
classifier
```

```{r }
test <- data.frame(income=c("high"), gender=c("male"))
test$income <- factor(test$income, levels=c("high","medium", "low"))
test$gender <- factor(test$gender, levels=c("male","female"))
prediction <- predict(classifier, test ,type="raw")
prediction
```

```{r }
#Next lab
#Let's load the Titanic training data. What are the odds of surviving the shipwreck?
library(dplyr)
library(ggplot2)
library(stargazer)
data <- read.csv("https://www.dropbox.com/s/eg6kack8wmlqmhg/titanic_train.csv?raw=1")
test_data <-read.csv("https://www.dropbox.com/s/eg6kack8wmlqmhg/titanic_test.csv?raw=1")
mean(data$Survived)
```

```{r }
#Using the logit model, estimate how much lower are the odds of survival for men relative to women?

logit1 <- glm(Survived ~ Sex, data=data, family="binomial")
summary(logit)
exp(coef(logit))
#Men's odds are 91.01 percent lower
```

```{r }
#Controlling for gender, does age have a statistically significant effect on the odds of survival? If so, what is the magnitude of that effect?

logit2 <- glm(Survived ~ Sex + Age, data=data, family="binomial")
summary(logit2)
exp(coef(logit))

```

```{r }
#Controlling for gender, does passenger class have a statistically significant effect on the odds of survival? If so, what is the magnitude of that effect?
logit3 <- glm(Survived ~ Pclass + Sex, data=data, family="binomial")
summary(logit3)
```

```{r }
#Controlling for gender, estimate the effect of being in the second class relative to first class, and the effect of being in the third relative to first.
logit4 <- glm(Survived ~ as.factor(Pclass) + Sex, data=data, family="binomial")
summary(logit4)

```

```{r }
#Add fare to the regression you estimated above. Is fare a significant determinant of survival controlling for gender and passenger class? Do you think that if we regressed survival on just gender and fare, fare would be significant? Explain.
logit5 <- glm(Survived ~ as.factor(Pclass) + Sex + Fare, data=data, family="binomial")
summary(logit5)
```

```{r }
#As we know from the movie, Jack traveled in the third class and paid 5 pounds (I know that Jack actually won the ticket in poker, but Swen, from whom Jack won the ticket, paid .). Rose traveled in the first class and paid 500 for her ticket (I know that her fiancee, Cal Hockley - Pittsburgh steel tycoon, actually bought the ticket, but .). What is the probability that Jack will survive? What is the probability that Rose will survive?

logit6 <- glm(Survived ~ as.factor(Pclass) + Sex + Fare, data=data, family="binomial")
jack <- data.frame(Sex="male", Pclass=3, Fare=5)
rose <- data.frame(Sex="female", Pclass=1,Fare=500)
test$jack <- predict(logit6, jack, type="response")
test$rose <- predict(logit6, rose, type="response")
test

```

```{r }
#Create your own logistic model and make predictions for passengers in the Titanic test data set. Keep in mind that you must make predictions for all passengers in the test data (even those with missing values). Use your own probability cut off for predicting survival (0.5 is a natural start). Submit your predictions to kaggle, did you do better with logistic regression than with decision trees? Which algorithm do you like better?

View(test_data)

```

```{r }

```

